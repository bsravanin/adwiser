GOALS
Why was an ad shown?
	Was it targeted on a D or a R/X?
	If targeted on a D, which D?
	Predict these while maximizing precision and recall, using minimal number of shadow accounts.


ASSUMPTIONS
If targeted on at least one email thread (found in ccloudauditor2* or not found in ccloudauditor1*): Ds
Else if targeted on os, browser, gmail, location, career, cars, money, music, future event (black friday): Xs
Else if rarely repeated with and across accounts: Rs


KNOWLEDGE
Some ads are targeted on Ds (emails).
Ratio of number of targeted ads and number of trials is a decreasing function.
Targeted ads are repeated across accounts with common Ds.
Xs are repeated across similar accounts even with no common Ds.


TODOS
Falses in predicting T-Vs-U and Ds.
	Why are precision and recall not 1?
	What are the common kinds of problems?
	(How) can they be overcome?
New accounts
	More data (real vs synthetic) -- Decided that it should be synthetic, non-ambiguous.
	New structure
		i accounts containing D and not containing D, i = 1, 2, 3, ... ?
		"Load balanced"
		Fix number of accounts? Number of emails per account? Something else? What are the properties?
		Randomness is good.
		Permutations, simulations, whatever works and is easy/simple to try out.
		GRAPH Performance Vs varying number of shadow accounts. (Same (alpha, beta, threshold)?)
		Goal is to find the minimum number of accounts we can get away with.
New avenues
	Without logging in (without accounts).
	Is history impacting ads (not just results)?
	May need to use different machines.
	Web searches (Are web searches useful to determine targeted Vs untargeted?)
	Browsing history (Ads in other pages)
	GOOD IDEA NOT FEASIBLE: To check whether ads are targeted on some content, post the content in a new private/empty circle. Are there private pages with ads?

LOWPRI TODOS
PENDING When do and don't precision Vs recall graphs make sense.
GRAPH Performance Vs varying number of shadow trials. (Same (alpha, beta, threshold)?)
What other good scoring models are possible? (Weighted models.)
Improve ad matching. May be more complicated, but to increase precision while not sacrificing recall.
Selenium parallel.
Are ads a function of time or number of access of an email?


DONE
1. Ad matching: find precision and recall.
	foreach (ad1, ad2), 1 if ads are same and 0 otherwise.
	90% precision and 80% recall is a good matching model.
	If good matching model, ignore ad matching.
	Matching with only exact match of AdURL/Text: 487 TPs, 53 FPs, 28 FNs
	Precision: 0.9018518518518519, Recall: 0.945631067961165
2. ads Vs trials graph.
	Knee around 33 trials for ccloudauditor10-30.
	Knees 3, 4 for ccloudauditor, ccloudauditor2.
3. Common ads in identical accounts.
4. Common ads in disjoint accounts.
5. Order by recall, not precision. Recall is a monotonic function. Precision is not.
6. Full truth, including types.
7. Targeted ads Vs Trials, TargetedX ads Vs Trials, Random Vs Trials graphs.
8. Ad types in identical accounts.
9. Verified whether varying the subset (or order) of trials varies the knee. (Say, in identical accounts.)
10. Cleanup page on disjoint accounts, including more useful graphs. One graph of all (1x, 2x) combinations and another of all (2x, 1x) combinations.
11. Various graphs, including precision Vs recall:
		foreach (alpha, beta, model) threshold on x-axis.
		foreach (threshold, alpha, model) beta on x-axis.
		foreach (threshold, beta, model) alpha on x-axis.
12. New disjoint accounts, threads, trials, truth, comparisons.
13. Models with all combinations of (alpha, beta), (a, b, c, d). All models give identical results.
14. Aggregation models. Normalizing scores a big problem, and could be the cause of bad results.
15. Aggregation and exponentiation models using both penalties and both rewards.
16. NO MORE NORMALIZATION.
17. Parallelized computation of precision and recall for various model, alpha, beta, threshold combinations. (Threshold now a function of alpha and beta.)
18. Ad dumps for individual base, cumulative shadow, common + diff.
19. Precision and recall for various model, alpha, beta, threshold combinations for each base trials.
20. A more methodical manner of identifying optimal parameters. Based on area under (precision+recall limit) and base trials.
21. Done with (alpha, beta, thresholds) for all models.
22. Equivalence of models.
23. Predict Ds.
24. Nico Viennot implemented a way to create many accounts using mechanical turks.
25. Average precision and recall across base trials.
26. Optimal parameters using F-measure.

LOWPRI DONE
1. Describe args and returns along with the purpose for all functions.
2. Area under precision and recall curve is the efficiency of a model.
